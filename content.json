{"meta":{"title":"Siqin Dong","subtitle":"","description":"","author":"Siqin Dong","url":"https://siqindong.github.io","root":"/"},"pages":[{"title":"ÂÖ≥‰∫é","date":"2020-08-07T01:56:26.494Z","updated":"2020-08-05T22:07:49.385Z","comments":false,"path":"about/index.html","permalink":"https://siqindong.github.io/about/index.html","excerpt":"","text":"‰∏™‰∫∫ËØ¶ÁªÜ‰ªãÁªç"},{"title":"‰π¶Âçï","date":"2020-08-07T01:56:26.503Z","updated":"2020-08-05T22:07:49.385Z","comments":false,"path":"books/index.html","permalink":"https://siqindong.github.io/books/index.html","excerpt":"","text":""},{"title":"ÂàÜÁ±ª","date":"2020-08-07T01:56:26.531Z","updated":"2020-08-05T22:07:49.386Z","comments":false,"path":"categories/index.html","permalink":"https://siqindong.github.io/categories/index.html","excerpt":"","text":""},{"title":"ÂèãÊÉÖÈìæÊé•","date":"2020-08-07T01:56:26.518Z","updated":"2020-08-05T22:07:49.386Z","comments":true,"path":"links/index.html","permalink":"https://siqindong.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-08-06T00:56:48.365Z","updated":"2020-08-05T22:07:49.386Z","comments":false,"path":"repository/index.html","permalink":"https://siqindong.github.io/repository/index.html","excerpt":"","text":""},{"title":"Ê†áÁ≠æ","date":"2020-08-07T01:56:26.545Z","updated":"2020-08-05T22:07:49.387Z","comments":false,"path":"tags/index.html","permalink":"https://siqindong.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Code highlight test","slug":"Code-highlight-test","date":"2020-08-06T04:27:24.000Z","updated":"2020-08-06T05:38:06.780Z","comments":true,"path":"2020/08/06/Code-highlight-test/","link":"","permalink":"https://siqindong.github.io/2020/08/06/Code-highlight-test/","excerpt":"","text":"This article is an export of the Gaussian processes notebook which is part of the bayesian-machine-learning repo on Github. Introduction In supervised learning, we often use parametric models p(y‚à£X,Œ∏)p(\\mathbf{y} \\lvert \\mathbf{X},\\boldsymbol\\theta)p(y‚à£X,Œ∏) to explain data and infer optimal values of parameter Œ∏\\boldsymbol\\thetaŒ∏ via maximum likelihood or maximum a posteriori estimation. If needed we can also infer a full posterior distribution p(Œ∏‚à£X,y)p(\\boldsymbol\\theta \\lvert \\mathbf{X},\\mathbf{y})p(Œ∏‚à£X,y) instead of a point estimate Œ∏^\\boldsymbol{\\hat\\theta}Œ∏^. With increasing data complexity, models with a higher number of parameters are usually needed to explain data reasonably well. Methods that use models with a fixed number of parameters are called parametric methods. In non-parametric methods, on the other hand, the number of parameters depend on the dataset size. For example, in Nadaraya-Watson kernel regression, a weight wiw_iwi‚Äã is assigned to each observed target yiy_iyi‚Äã and for predicting the target value at a new point x\\mathbf{x}x a weighted average is computed: f(x)=‚àëi=1Nwi(x)yif(\\mathbf{x}) = \\sum_{i=1}^{N}w_i(\\mathbf{x})y_i f(x)=i=1‚àëN‚Äãwi‚Äã(x)yi‚Äã wi(x)=Œ∫(x,xi)‚àëi‚Ä≤=1NŒ∫(x,xi‚Ä≤)w_i(\\mathbf{x}) = \\frac{\\kappa(\\mathbf{x}, \\mathbf{x}_{i})}{\\sum_{i&#x27;=1}^{N}\\kappa(\\mathbf{x}, \\mathbf{x}_{i&#x27;})} wi‚Äã(x)=‚àëi‚Ä≤=1N‚ÄãŒ∫(x,xi‚Ä≤‚Äã)Œ∫(x,xi‚Äã)‚Äã Observations that are closer to x\\mathbf{x}x have a higher weight than observations that are further away. Weights are computed from x\\mathbf{x}x and observed xi\\mathbf{x}_ixi‚Äã with a kernel Œ∫\\kappaŒ∫. A special case is k-nearest neighbors (KNN) where the kkk closest observations have a weight 1/k1/k1/k, and all others have weight 000. Non-parametric methods often need to process all training data for prediction and are therefore slower at inference time than parametric methods. On the other hand, training is usually faster as non-parametric models only need to remember training data. Another example of non-parametric methods are Gaussian processes (GPs). Instead of inferring a distribution over the parameters of a parametric function Gaussian processes can be used to infer a distribution over functions directly. A Gaussian process defines a prior over functions. After having observed some function values it can be converted into a posterior over functions. Inference of continuous function values in this context is known as GP regression but GPs can also be used for classification. A Gaussian process is a random process where any point x‚ààRd\\mathbf{x} \\in \\mathbb{R}^dx‚ààRd is assigned a random variable f(x)f(\\mathbf{x})f(x) and where the joint distribution of a finite number of these variables p(f(x1),...,f(xN))p(f(\\mathbf{x}_1),...,f(\\mathbf{x}_N))p(f(x1‚Äã),...,f(xN‚Äã)) is itself Gaussian: p(\\mathbf{f} \\lvert \\mathbf{X}) = \\mathcal{N}(\\mathbf{f} \\lvert \\boldsymbol\\mu, \\mathbf{K})\\tag{1}\\label{eq1} In Equation (1)(1)(1), $$\\mathbf{f} = (f(\\mathbf{x}_1),‚Ä¶,f(\\mathbf{x}_N))$$, $$\\boldsymbol\\mu = (m(\\mathbf{x}_1),‚Ä¶,m(\\mathbf{x}N))$$ and $$K{ij} = \\kappa(\\mathbf{x}_i,\\mathbf{x}_j)$$. mmm is the mean function and it is common to use m(x)=0m(\\mathbf{x}) = 0m(x)=0 as GPs are flexible enough to model the mean arbitrarily well. Œ∫\\kappaŒ∫ is a positive definite kernel function or covariance function. Thus, a Gaussian process is a distribution over functions whose shape (smoothness, ‚Ä¶) is defined by K\\mathbf{K}K. If points xi\\mathbf{x}_ixi‚Äã and xj\\mathbf{x}_jxj‚Äã are considered to be similar by the kernel the function values at these points, f(xi)f(\\mathbf{x}_i)f(xi‚Äã) and f(xj)f(\\mathbf{x}_j)f(xj‚Äã), can be expected to be similar too. A GP prior p(f‚à£X)p(\\mathbf{f} \\lvert \\mathbf{X})p(f‚à£X) can be converted into a GP posterior p(f‚à£X,y)p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})p(f‚à£X,y) after having observed some data y\\mathbf{y}y. The posterior can then be used to make predictions $$\\mathbf{f}*$$ given new input $$\\mathbf{X}*$$: \\begin{align*} p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{X},\\mathbf{y}) &= \\int{p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{f})p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})}\\ d\\mathbf{f} \\\\ &= \\mathcal{N}(\\mathbf{f}_* \\lvert \\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)\\tag{2}\\label{eq2} \\end{align*} Equation (2)(2)(2) is the posterior predictive distribution which is also a Gaussian with mean $$\\boldsymbol{\\mu}*$$ and $$\\boldsymbol{\\Sigma}$$. By definition of the GP, the joint distribution of observed data y\\mathbf{y}y and predictions $$\\mathbf{f}_$$ is \\begin{pmatrix}\\mathbf{y} \\\\ \\mathbf{f}_*\\end{pmatrix} \\sim \\mathcal{N} \\left(\\boldsymbol{0}, \\begin{pmatrix}\\mathbf{K}_y & \\mathbf{K}_* \\\\ \\mathbf{K}_*^T & \\mathbf{K}_{**}\\end{pmatrix} \\right)\\tag{3}\\label{eq3} With NNN training data and $$N_$$ new input data, $$\\mathbf{K}y = \\kappa(\\mathbf{X},\\mathbf{X}) + \\sigma_y^2\\mathbf{I} = \\mathbf{K} + \\sigma_y^2\\mathbf{I}$$ is $$N \\times N$$, $$\\mathbf{K} = \\kappa(\\mathbf{X},\\mathbf{X}*)$$ is $$N \\times N$$ and $$\\mathbf{K}{**} = \\kappa(\\mathbf{X},\\mathbf{X}*)$$ is $$N* \\times N_$$. œÉy2\\sigma_y^2œÉy2‚Äã is the noise term in the diagonal of Ky\\mathbf{K_y}Ky‚Äã. It is set to zero if training targets are noise-free and to a value greater than zero if observations are noisy. The mean is set to 0\\boldsymbol{0}0 for notational simplicity. The sufficient statistics of the posterior predictive distribution, $$\\boldsymbol{\\mu}_$$ and $$\\boldsymbol{\\Sigma}_*$$, can be computed with[1][3] \\begin{align*} \\boldsymbol{\\mu_*} &= \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{y}\\tag{4}\\label{eq4} \\\\ \\boldsymbol{\\Sigma_*} &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{K}_*\\tag{5}\\label{eq5} \\end{align*} This is the minimum we need to know for implementing Gaussian processes and applying them to regression problems. For further details, please consult the literature in the References section. The next section shows how to implement GPs with plain NumPy from scratch, later sections demonstrate how to use GP implementations from scikit-learn and GPy. Implementation with NumPy Here, we will use the squared exponential kernel, also known as Gaussian kernel or RBF kernel: Œ∫(xi,xj)=œÉf2exp‚Å°(‚àí12l2(xi‚àíxj)T(xi‚àíxj))(6)\\kappa(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma_f^2 \\exp(-\\frac{1}{2l^2} (\\mathbf{x}_i - \\mathbf{x}_j)^T (\\mathbf{x}_i - \\mathbf{x}_j))\\tag{6} Œ∫(xi‚Äã,xj‚Äã)=œÉf2‚Äãexp(‚àí2l21‚Äã(xi‚Äã‚àíxj‚Äã)T(xi‚Äã‚àíxj‚Äã))(6) The length parameter lll controls the smoothness of the function and œÉf\\sigma_fœÉf‚Äã the vertical variation. For simplicity, we use the same length parameter lll for all input dimensions (isotropic kernel). 12345678910111213141516import numpy as npdef kernel(X1, X2, l=1.0, sigma_f=1.0): &#x27;&#x27;&#x27; Isotropic squared exponential kernel. Computes a covariance matrix from points in X1 and X2. Args: X1: Array of m points (m x d). X2: Array of n points (n x d). Returns: Covariance matrix (m x n). &#x27;&#x27;&#x27; sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T) return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist) There are many other kernels that can be used for Gaussian processes. See [3] for a detailed reference or the scikit-learn documentation for some examples. Prior Let‚Äôs first define a prior over functions with mean zero and a covariance matrix computed with kernel parameters l=1l=1l=1 and œÉf=1\\sigma_f=1œÉf‚Äã=1. To draw random functions from that GP we draw random samples from the corresponding multivariate normal. The following example draws three random samples and plots it together with the zero mean and the 95% confidence interval (computed from the diagonal of the covariance matrix). 12345678910111213141516%matplotlib inlinefrom gaussian_processes_util import plot_gp# Finite number of pointsX = np.arange(-5, 5, 0.2).reshape(-1, 1)# Mean and covariance of the priormu = np.zeros(X.shape)cov = kernel(X, X)# Draw three samples from the priorsamples = np.random.multivariate_normal(mu.ravel(), cov, 3)# Plot GP mean, confidence interval and samples plot_gp(mu, cov, X, samples=samples) The plot_gp function is defined here. Prediction from noise-free training data To compute the sufficient statistics i.e. mean and covariance of the posterior predictive distribution we implement Equations (4)(4)(4) and (5)(5)(5) 123456789101112131415161718192021222324252627282930from numpy.linalg import invdef posterior_predictive(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8): &#x27;&#x27;&#x27; Computes the suffifient statistics of the GP posterior predictive distribution from m training data X_train and Y_train and n new inputs X_s. Args: X_s: New input locations (n x d). X_train: Training locations (m x d). Y_train: Training targets (m x 1). l: Kernel length parameter. sigma_f: Kernel vertical variation parameter. sigma_y: Noise parameter. Returns: Posterior mean vector (n x d) and covariance matrix (n x n). &#x27;&#x27;&#x27; K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train)) K_s = kernel(X_train, X_s, l, sigma_f) K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s)) K_inv = inv(K) # Equation (4) mu_s = K_s.T.dot(K_inv).dot(Y_train) # Equation (5) cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s) return mu_s, cov_s and apply them to noise-free training data X_train and Y_train. The following example draws three samples from the posterior predictive and plots them along with the mean, confidence interval and training data. In a noise-free model, variance at the training points is zero and all random functions drawn from the posterior go through the trainig points. 123456789# Noise free training dataX_train = np.array([-4, -3, -2, -1, 1]).reshape(-1, 1)Y_train = np.sin(X_train)# Compute mean and covariance of the posterior predictive distributionmu_s, cov_s = posterior_predictive(X, X_train, Y_train)samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, 3)plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train, samples=samples) Prediction from noisy training data If some noise is included in the model, training points are only approximated and the variance at the training points is non-zero. 1234567891011noise = 0.4# Noisy training dataX_train = np.arange(-3, 4, 1).reshape(-1, 1)Y_train = np.sin(X_train) + noise * np.random.randn(*X_train.shape)# Compute mean and covariance of the posterior predictive distributionmu_s, cov_s = posterior_predictive(X, X_train, Y_train, sigma_y=noise)samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, 3)plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train, samples=samples) Effect of kernel parameters and noise parameter The following example shows the effect of kernel parameters lll and œÉf\\sigma_fœÉf‚Äã as well as the noise parameter œÉy\\sigma_yœÉy‚Äã. Higher lll values lead to smoother functions and therefore to coarser approximations of the training data. Lower lll values make functions more wiggly with wide confidence intervals between training data points. œÉf\\sigma_fœÉf‚Äã controls the vertical variation of functions drawn from the GP. This can be seen by the wide confidence intervals outside the training data region in the right figure of the second row. œÉy\\sigma_yœÉy‚Äã represents the amount of noise in the training data. Higher œÉy\\sigma_yœÉy‚Äã values make more coarse approximations which avoids overfitting to noisy data. 123456789101112131415161718192021import matplotlib.pyplot as pltparams = [ (0.3, 1.0, 0.2), (3.0, 1.0, 0.2), (1.0, 0.3, 0.2), (1.0, 3.0, 0.2), (1.0, 1.0, 0.05), (1.0, 1.0, 1.5),]plt.figure(figsize=(12, 5))for i, (l, sigma_f, sigma_y) in enumerate(params): mu_s, cov_s = posterior_predictive(X, X_train, Y_train, l=l, sigma_f=sigma_f, sigma_y=sigma_y) plt.subplot(3, 2, i + 1) plt.subplots_adjust(top=2) plt.title(f&#x27;l = &#123;l&#125;, sigma_f = &#123;sigma_f&#125;, sigma_y = &#123;sigma_y&#125;&#x27;) plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train) Optimal values for these parameters can be estimated by maximizing the log marginal likelihood which is given by[1][3] log‚Å°p(y‚à£X)=log‚Å°N(y‚à£0,Ky)=‚àí12yTKy‚àí1y‚àí12log‚Å°‚à£Ky‚à£‚àíN2log‚Å°(2œÄ)(7)\\log p(\\mathbf{y} \\lvert \\mathbf{X}) = \\log \\mathcal{N}(\\mathbf{y} \\lvert \\boldsymbol{0},\\mathbf{K}_y) = -\\frac{1}{2} \\mathbf{y}^T \\mathbf{K}_y^{-1} \\mathbf{y} -\\frac{1}{2} \\log \\begin{vmatrix}\\mathbf{K}_y\\end{vmatrix} -\\frac{N}{2} \\log(2\\pi) \\tag{7} logp(y‚à£X)=logN(y‚à£0,Ky‚Äã)=‚àí21‚ÄãyTKy‚àí1‚Äãy‚àí21‚Äãlog‚à£‚à£‚ÄãKy‚Äã‚Äã‚à£‚à£‚Äã‚àí2N‚Äãlog(2œÄ)(7) In the following we will minimize the negative log marginal likelihood w.r.t. parameters lll and œÉf\\sigma_fœÉf‚Äã, œÉy\\sigma_yœÉy‚Äã is set to the known noise level of the data. If the noise level is unknown, œÉy\\sigma_yœÉy‚Äã can be estimated as well along with the other parameters. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061from numpy.linalg import cholesky, det, lstsqfrom scipy.optimize import minimizedef nll_fn(X_train, Y_train, noise, naive=True): &#x27;&#x27;&#x27; Returns a function that computes the negative log marginal likelihood for training data X_train and Y_train and given noise level. Args: X_train: training locations (m x d). Y_train: training targets (m x 1). noise: known noise level of Y_train. naive: if True use a naive implementation of Eq. (7), if False use a numerically more stable implementation. Returns: Minimization objective. &#x27;&#x27;&#x27; def nll_naive(theta): # Naive implementation of Eq. (7). Works well for the examples # in this article but is numerically less stable compared to # the implementation in nll_stable below. K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\ noise**2 * np.eye(len(X_train)) return 0.5 * np.log(det(K)) + \\ 0.5 * Y_train.T.dot(inv(K).dot(Y_train)) + \\ 0.5 * len(X_train) * np.log(2*np.pi) def nll_stable(theta): # Numerically more stable implementation of Eq. (7) as described # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section # 2.2, Algorithm 2.1. K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\ noise**2 * np.eye(len(X_train)) L = cholesky(K) return np.sum(np.log(np.diagonal(L))) + \\ 0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\ 0.5 * len(X_train) * np.log(2*np.pi) if naive: return nll_naive else: return nll_stable# Minimize the negative log-likelihood w.r.t. parameters l and sigma_f.# We should actually run the minimization several times with different# initializations to avoid local minima but this is skipped here for# simplicity.res = minimize(nll_fn(X_train, Y_train, noise), [1, 1], bounds=((1e-5, None), (1e-5, None)), method=&#x27;L-BFGS-B&#x27;)# Store the optimization results in global variables so that we can# compare it later with the results from other implementations.l_opt, sigma_f_opt = res.xl_opt, sigma_f_opt# Compute the prosterior predictive statistics with optimized kernel parameters and plot the resultsmu_s, cov_s = posterior_predictive(X, X_train, Y_train, l=l_opt, sigma_f=sigma_f_opt, sigma_y=noise)plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train) With optimized kernel parameters, training data are reasonably covered by the 95% confidence interval and the mean of the posterior predictive is a good approximation. Higher dimensions The above implementation can also be used for higher input data dimensions. Here, a GP is used to fit noisy samples from a sine wave originating at 0\\boldsymbol{0}0 and expanding in the x-y plane. The following plots show the noisy samples and the posterior predictive mean before and after kernel parameter optimization. 1234567891011121314151617181920212223242526from gaussian_processes_util import plot_gp_2Dnoise_2D = 0.1rx, ry = np.arange(-5, 5, 0.3), np.arange(-5, 5, 0.3)gx, gy = np.meshgrid(rx, rx)X_2D = np.c_[gx.ravel(), gy.ravel()]X_2D_train = np.random.uniform(-4, 4, (100, 2))Y_2D_train = np.sin(0.5 * np.linalg.norm(X_2D_train, axis=1)) + \\ noise_2D * np.random.randn(len(X_2D_train))plt.figure(figsize=(14,7))mu_s, _ = posterior_predictive(X_2D, X_2D_train, Y_2D_train, sigma_y=noise_2D)plot_gp_2D(gx, gy, mu_s, X_2D_train, Y_2D_train, f&#x27;Before parameter optimization: l=&#123;1.00&#125; sigma_f=&#123;1.00&#125;&#x27;, 1)res = minimize(nll_fn(X_2D_train, Y_2D_train, noise_2D), [1, 1], bounds=((1e-5, None), (1e-5, None)), method=&#x27;L-BFGS-B&#x27;)mu_s, _ = posterior_predictive(X_2D, X_2D_train, Y_2D_train, *res.x, sigma_y=noise_2D)plot_gp_2D(gx, gy, mu_s, X_2D_train, Y_2D_train, f&#x27;After parameter optimization: l=&#123;res.x[0]:.2f&#125; sigma_f=&#123;res.x[1]:.2f&#125;&#x27;, 2) Note how the true sine wave is approximated much better after parameter optimization. Libraries that implement GPs This section shows two examples of libraries that provide implementations of GPs. I‚Äôll provide only a minimal setup here, just enough for reproducing the above results. For further details please consult the documentation of these libraries. Scikit-learn Scikit-learn provides a GaussianProcessRegressor for implementing GP regression models. It can be configured with pre-defined kernels and user-defined kernels. Kernels can also be composed. The squared exponential kernel is the RBF kernel in scikit-learn. The RBF kernel only has a length_scale parameter which corresponds to the lll parameter above. To have a œÉf\\sigma_fœÉf‚Äã parameter as well, we have to compose the RBF kernel with a ConstantKernel. 12345678910111213141516171819202122from sklearn.gaussian_process import GaussianProcessRegressorfrom sklearn.gaussian_process.kernels import ConstantKernel, RBFrbf = ConstantKernel(1.0) * RBF(length_scale=1.0)gpr = GaussianProcessRegressor(kernel=rbf, alpha=noise**2)# Reuse training data from previous 1D examplegpr.fit(X_train, Y_train)# Compute posterior predictive mean and covariancemu_s, cov_s = gpr.predict(X, return_cov=True)# Obtain optimized kernel parametersl = gpr.kernel_.k2.get_params()[&#x27;length_scale&#x27;]sigma_f = np.sqrt(gpr.kernel_.k1.get_params()[&#x27;constant_value&#x27;])# Compare with previous resultsassert(np.isclose(l_opt, l))assert(np.isclose(sigma_f_opt, sigma_f))# Plot the resultsplot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train) GPy GPy is a Gaussian processes framework from the Sheffield machine learning group. It provides a GPRegression class for implementing GP regression models. By default, GPRegression also estimates the noise parameter œÉy\\sigma_yœÉy‚Äã from data, so we have to fix() this parameter to be able to reproduce the above results. 12345678910111213141516171819202122import GPyrbf = GPy.kern.RBF(input_dim=1, variance=1.0, lengthscale=1.0)gpr = GPy.models.GPRegression(X_train, Y_train, rbf)# Fix the noise variance to known value gpr.Gaussian_noise.variance = noise**2gpr.Gaussian_noise.variance.fix()# Run optimizationgpr.optimize();# Obtain optimized kernel parametersl = gpr.rbf.lengthscale.values[0]sigma_f = np.sqrt(gpr.rbf.variance.values[0])# Compare with previous resultsassert(np.isclose(l_opt, l))assert(np.isclose(sigma_f_opt, sigma_f))# Plot the results with the built-in plot functiongpr.plot(); UserWarning:This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. Thanks for reading up to here üòÉ In another article, I‚Äôll show how Gaussian processes can be used for black-box optimization. References [1] Kevin P. Murphy. Machine Learning, A Probabilistic Perspective, Chapters 4, 14 and 15. [2] Christopher M. Bishop. Pattern Recognition and Machine Learning, Chapter 6. [3] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.","categories":[],"tags":[{"name":"test","slug":"test","permalink":"https://siqindong.github.io/tags/test/"}]},{"title":"Math test","slug":"Math-test","date":"2020-08-06T00:52:43.000Z","updated":"2020-08-06T05:38:06.784Z","comments":true,"path":"2020/08/05/Math-test/","link":"","permalink":"https://siqindong.github.io/2020/08/05/Math-test/","excerpt":"","text":"Math test This a math test Introduction In supervised learning, we often use parametric models p(y‚à£X,Œ∏)p(\\mathbf{y} \\lvert \\mathbf{X},\\boldsymbol\\theta)p(y‚à£X,Œ∏) to explain data and infer optimal values of parameter Œ∏\\boldsymbol\\thetaŒ∏ via maximum likelihood or maximum a posteriori estimation. If needed we can also infer a full posterior distribution p(Œ∏‚à£X,y)p(\\boldsymbol\\theta \\lvert \\mathbf{X},\\mathbf{y})p(Œ∏‚à£X,y) instead of a point estimate Œ∏^\\boldsymbol{\\hat\\theta}Œ∏^. With increasing data complexity, models with a higher number of parameters are usually needed to explain data reasonably well. Methods that use models with a fixed number of parameters are called parametric methods. In non-parametric methods, on the other hand, the number of parameters depend on the dataset size. For example, in Nadaraya-Watson kernel regression, a weight wiw_iwi‚Äã is assigned to each observed target yiy_iyi‚Äã and for predicting the target value at a new point x\\mathbf{x}x a weighted average is computed: f(x)=‚àëi=1Nwi(x)yif(\\mathbf{x}) = \\sum_{i=1}^{N}w_i(\\mathbf{x})y_i f(x)=i=1‚àëN‚Äãwi‚Äã(x)yi‚Äã wi(x)=Œ∫(x,xi)‚àëi‚Ä≤=1NŒ∫(x,xi‚Ä≤)w_i(\\mathbf{x}) = \\frac{\\kappa(\\mathbf{x}, \\mathbf{x}_{i})}{\\sum_{i&#x27;=1}^{N}\\kappa(\\mathbf{x}, \\mathbf{x}_{i&#x27;})} wi‚Äã(x)=‚àëi‚Ä≤=1N‚ÄãŒ∫(x,xi‚Ä≤‚Äã)Œ∫(x,xi‚Äã)‚Äã","categories":[],"tags":[{"name":"test","slug":"test","permalink":"https://siqindong.github.io/tags/test/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-08-06T00:13:00.486Z","updated":"2020-08-06T03:12:48.897Z","comments":true,"path":"2020/08/05/hello-world/","link":"","permalink":"https://siqindong.github.io/2020/08/05/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"test","slug":"test","permalink":"https://siqindong.github.io/tags/test/"}]}